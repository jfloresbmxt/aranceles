{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_intro_hts",
   "metadata": {},
   "source": [
    "# Limpieza HTS\n",
    "## Fases de Tratado\n",
    "### Fase 0: Configuración General\n",
    "\n",
    "El objetivo de este script es procesar la base de datos de tarifas de Estados Unidos (HTS - Harmonized Tariff Schedule). Integra una extracción dinámica de la estructura (Secciones y Capítulos) directamente desde el PDF oficial y cruza esta información con los datos tarifarios crudos.\n",
    "\n",
    "Instalación de dependencias:\n",
    "- `pip install pdfplumber pandas xlsxwriter openpyxl`\n",
    "\n",
    "Librerías:\n",
    "- **pdfplumber:** Motor de extracción para leer la jerarquía del PDF `Table of Contents`.\n",
    "- **pandas (pd):** Manipulación de estructuras de datos (DataFrames).\n",
    "- **re:** Expresiones regulares para detectar patrones de texto (Secciones, Capítulos, Notas).\n",
    "- **xlsxwriter:** Motor de escritura para Excel con formato avanzado.\n",
    "- **os/sys:** Manejo de rutas y control de flujo.\n",
    "\n",
    "Variables Globales:\n",
    "- **Rutas:** Apuntan a los archivos Raw (CSV y PDF) y definen la salida procesada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "code_config_hts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURACIÓN CARGADA ---\n",
      "Input CSV: ../data/raw/htsdata.csv\n",
      "Input PDF: ../data/raw/Table of Contents_2025HTSRev31.pdf\n",
      "Output:    ../data/intermediate/HTS_Limpia.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PATH_HTS_CSV = r\"../data/raw/htsdata.csv\"\n",
    "PATH_HTS_PDF = r\"../data/raw/Table of Contents_2025HTSRev31.pdf\"\n",
    "PATH_SALIDA = \"../data/intermediate/HTS_Limpia.xlsx\"\n",
    "\n",
    "print(\"--- CONFIGURACIÓN CARGADA ---\")\n",
    "print(f\"Input CSV: {PATH_HTS_CSV}\")\n",
    "print(f\"Input PDF: {PATH_HTS_PDF}\")\n",
    "print(f\"Output:    {PATH_SALIDA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_defs_hts",
   "metadata": {},
   "source": [
    "### Fase 0.5: Definición de Funciones\n",
    "\n",
    "Se definen todas las funciones del sistema de manera plana (sin anidamiento) para facilitar su depuración y mantenimiento. A continuación se explica cada una:\n",
    "\n",
    "**1. Funciones Auxiliares (`_nombre`)**\n",
    "Operaciones atómicas de limpieza y formateo.\n",
    "\n",
    "- **`_normalizar_texto`**: Recibe un texto crudo (ej. \"LIVE ANIMALS\"), lo convierte a formato Oración (\"Live animals\") y asegura que termine en punto final.\n",
    "- **`_restaurar_puntos`**: Convierte un string numérico limpio (`01012100`) al formato visual HTS con puntos (`0101.21.00`).\n",
    "- **`_concatenar_descripciones`**: Toma las columnas jerárquicas (Sección, Capítulo, Partida, etc.) de una fila y las une en un solo párrafo coherente.\n",
    "- **`_registrar_capitulo`**: Función de apoyo para el procesador PDF. Se encarga de guardar en la lista principal el Capítulo y Sección que se acaban de terminar de leer, uniendo las partes multilínea del título.\n",
    "\n",
    "**2. Funciones Críticas (`__nombre__`)**\n",
    "Motores de lectura y escritura de archivos.\n",
    "\n",
    "- **`__procesar_pdf_estructura__`**: Implementa una Máquina de Estados para leer el PDF línea por línea. Detecta cambios de Sección y Capítulo, acumula líneas de títulos largos y utiliza `_registrar_capitulo` para guardar los avances.\n",
    "- **`__escribir_hoja__`**: Controlador de `xlsxwriter`. Genera las pestañas de Excel, configura los encabezados y aplica lógica condicional para el ajuste de texto (Wrap Text) en descripciones y tasas especiales.\n",
    "\n",
    "**3. Funciones Principales (`nombre`)**\n",
    "Orquestadores de la lógica de negocio.\n",
    "\n",
    "- **`construir_diccionarios_maestros`**: Transforma el DataFrame extraído del PDF en diccionarios optimizados (Hash Maps) para cruzar información rápidamente por código.\n",
    "- **`procesar_csv_hts`**: Carga la base de datos tarifaria, elimina el Capítulo 99 (administrativo) y ejecuta la lógica de Herencia de Indentación (donde los códigos hijos heredan los dígitos de los padres).\n",
    "- **`generar_paneles`**: Cruza la información del CSV procesado con los diccionarios del PDF. Normaliza los textos y genera las columnas derivadas necesarias para los reportes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "code_defs_hts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIONES AUXILIARES ---\n",
    "\n",
    "def _normalizar_texto(texto):\n",
    "    \"\"\"Capitaliza (Oración) y agrega punto final si falta.\"\"\"\n",
    "    t = str(texto).strip()\n",
    "    if not t or t.lower() == 'nan': return \"\"\n",
    "    t = t[0].upper() + t[1:].lower()\n",
    "    if not t.endswith('.'):\n",
    "        t += \".\"\n",
    "    return t\n",
    "\n",
    "\n",
    "def _restaurar_puntos(code):\n",
    "    \"\"\"Formato visual HTS (xxxx.xx.xx).\"\"\"\n",
    "    s = str(code)\n",
    "    if len(s) <= 4: return s\n",
    "    elif len(s) <= 6: return f\"{s[:4]}.{s[4:]}\"\n",
    "    elif len(s) <= 8: return f\"{s[:4]}.{s[4:6]}.{s[6:]}\"\n",
    "    return f\"{s[:4]}.{s[4:6]}.{s[6:8]}.{s[8:]}\"\n",
    "\n",
    "\n",
    "def _concatenar_descripciones(row):\n",
    "    \"\"\"Une jerarquía textual en una sola cadena limpia.\"\"\"\n",
    "    partes = [\n",
    "        row.get('Seccion_Nombre', ''),\n",
    "        row.get('Capitulo_Nom', ''),\n",
    "        row.get('Partida_Nom', ''),\n",
    "        row.get('Desdoblamiento_Nom', ''),\n",
    "        row.get('Subpartida_Nom', ''),\n",
    "        row.get('Description', '')\n",
    "    ]\n",
    "    validas = [str(x).strip() for x in partes if str(x).strip() not in ['', 'nan', 'ND', 'None']]\n",
    "    return \" \".join(validas)\n",
    "\n",
    "\n",
    "def _registrar_capitulo(data_list, sec_id, sec_parts, chap_id, chap_parts):\n",
    "    \"\"\"Guarda el capítulo actual en la lista de datos.\"\"\"\n",
    "    if chap_id is not None:\n",
    "        s_name = \" \".join(sec_parts).strip()\n",
    "        c_name = \" \".join(chap_parts).strip()\n",
    "        data_list.append({\n",
    "            \"Seccion_ID\": sec_id,\n",
    "            \"Seccion_Nombre\": s_name,\n",
    "            \"Capitulo_ID\": chap_id,\n",
    "            \"Capitulo_Nombre\": c_name\n",
    "        })\n",
    "\n",
    "\n",
    "# --- FUNCIONES CRÍTICAS ---\n",
    "\n",
    "def __procesar_pdf_estructura__(pdf_path):\n",
    "    \"\"\"\n",
    "    Motor de extracción PDF basado en Máquina de Estados.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        sys.exit(f\"ERROR CRÍTICO: No se encontró el PDF en: {pdf_path}\")\n",
    "\n",
    "    print(f\">> [CRÍTICO] Leyendo estructura desde PDF...\")\n",
    "    data = []\n",
    "    \n",
    "    # Variables de estado\n",
    "    last_sec_id = \"\"\n",
    "    last_sec_name_parts = []\n",
    "    last_chap_id = None\n",
    "    last_chap_name_parts = []\n",
    "    next_expected_chap = 1\n",
    "    \n",
    "    # Modos\n",
    "    MODE_SEARCH, MODE_READING_SEC, MODE_READING_CHAP = 0, 1, 2\n",
    "    current_mode = MODE_SEARCH\n",
    "\n",
    "    # Regex\n",
    "    rx_section = re.compile(r\"^SECTION\\s+([IVXLCDM]+)\", re.IGNORECASE)\n",
    "    rx_explicit_chap = re.compile(r\"^Chapter\\s+(\\d+)\", re.IGNORECASE)\n",
    "    rx_stop = re.compile(r\"^(Section Note|General Note|Page|Change Record|Annex|Rate of Duty|Statistical Annexes|Chemical Appendix|Pharmaceutical Appendix|Intermediate Chemicals)\", re.IGNORECASE)\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if not text: continue\n",
    "            for line in text.split('\\n'):\n",
    "                clean = line.strip()\n",
    "                if not clean: continue\n",
    "                \n",
    "                # 1. Stop Tokens\n",
    "                if rx_stop.match(clean) or clean.isdigit():\n",
    "                    if current_mode == MODE_READING_SEC: current_mode = MODE_SEARCH\n",
    "                    elif current_mode == MODE_READING_CHAP:\n",
    "                        if \"Note\" in clean or \"Appendix\" in clean or \"Annex\" in clean: current_mode = MODE_SEARCH\n",
    "                    continue\n",
    "\n",
    "                # 2. Nueva Sección\n",
    "                sec_match = rx_section.match(clean)\n",
    "                if sec_match:\n",
    "                    # Guardar anterior\n",
    "                    _registrar_capitulo(data, last_sec_id, last_sec_name_parts, last_chap_id, last_chap_name_parts)\n",
    "                    \n",
    "                    last_sec_id = f\"SECTION {sec_match.group(1)}\"\n",
    "                    last_sec_name_parts = []; last_chap_id = None; last_chap_name_parts = []\n",
    "                    current_mode = MODE_READING_SEC\n",
    "                    continue\n",
    "\n",
    "                # 3. Nuevo Capítulo\n",
    "                detected_num = None; text_start_idx = 0\n",
    "                explicit = rx_explicit_chap.match(clean)\n",
    "                if explicit:\n",
    "                    detected_num = int(explicit.group(1)); text_start_idx = explicit.end()\n",
    "                elif clean.split()[0].isdigit():\n",
    "                    first_word = clean.split()[0]\n",
    "                    if first_word.isdigit():\n",
    "                        detected_num = int(first_word); text_start_idx = len(first_word)\n",
    "\n",
    "                if detected_num is not None and detected_num == next_expected_chap:\n",
    "                    # Guardar anterior\n",
    "                    _registrar_capitulo(data, last_sec_id, last_sec_name_parts, last_chap_id, last_chap_name_parts)\n",
    "                    \n",
    "                    if current_mode == MODE_READING_SEC: current_mode = MODE_SEARCH\n",
    "                    last_chap_id = detected_num\n",
    "                    last_chap_name_parts = []\n",
    "                    rest_of_line = clean[text_start_idx:].strip()\n",
    "                    if rest_of_line: last_chap_name_parts.append(rest_of_line)\n",
    "                    next_expected_chap += 1\n",
    "                    current_mode = MODE_READING_CHAP\n",
    "                    continue\n",
    "\n",
    "                # 4. Acumulación\n",
    "                if current_mode == MODE_READING_SEC: last_sec_name_parts.append(clean)\n",
    "                elif current_mode == MODE_READING_CHAP: last_chap_name_parts.append(clean)\n",
    "\n",
    "    # Guardar último\n",
    "    _registrar_capitulo(data, last_sec_id, last_sec_name_parts, last_chap_id, last_chap_name_parts)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def __escribir_hoja__(dataframe, nombre_hoja, writer, workbook):\n",
    "    \"\"\"\n",
    "    Motor de escritura Excel con Wrap Text condicional.\n",
    "    \"\"\"\n",
    "    worksheet = workbook.add_worksheet(nombre_hoja)\n",
    "    writer.sheets[nombre_hoja] = worksheet\n",
    "    \n",
    "    # Formatos\n",
    "    fmt_header = workbook.add_format({'bold': True, 'border': 1, 'bg_color': '#D9E1F2', 'valign': 'top', 'align': 'center', 'font_name': 'Arial', 'font_size': 10})\n",
    "    fmt_normal = workbook.add_format({'border': 1, 'valign': 'top', 'font_name': 'Arial', 'font_size': 9})\n",
    "    fmt_wrap = workbook.add_format({'border': 1, 'valign': 'top', 'text_wrap': True, 'font_name': 'Arial', 'font_size': 9})\n",
    "\n",
    "    headers = dataframe.columns.tolist()\n",
    "    \n",
    "    # Encabezados\n",
    "    for col_idx, header in enumerate(headers):\n",
    "        worksheet.write(0, col_idx, header, fmt_header)\n",
    "    \n",
    "    # Cuerpo\n",
    "    for row_idx, row in enumerate(dataframe.itertuples(index=False), start=1):\n",
    "        for col_idx, value in enumerate(row):\n",
    "            header_name = str(headers[col_idx])\n",
    "            \n",
    "            # Lógica Wrap Text: Descripciones, Nombres, Concatenado y Special Rate\n",
    "            es_wrap = any(x in header_name for x in [\"Name\", \"Description\", \"Special\", \"Concatenated\"])\n",
    "            formato = fmt_wrap if es_wrap else fmt_normal\n",
    "            \n",
    "            if pd.isna(value): value = \"\"\n",
    "            worksheet.write(row_idx, col_idx, value, formato)\n",
    "\n",
    "    # Ajuste de Anchos\n",
    "    for idx, col_name in enumerate(headers):\n",
    "        header_str = str(col_name)\n",
    "        if any(x in header_str for x in [\"Name\", \"Description\", \"Concatenated\", \"Special\"]):\n",
    "            worksheet.set_column(idx, idx, 50) # Ancho fijo amplio para Wrap\n",
    "        else:\n",
    "            worksheet.set_column(idx, idx, 15) # Ancho estándar\n",
    "\n",
    "\n",
    "# --- FUNCIONES PRINCIPALES ---\n",
    "\n",
    "def construir_diccionarios_maestros(df_pdf):\n",
    "    print(\">> Construyendo diccionarios maestros...\")\n",
    "    df_pdf['Seccion_Romana'] = df_pdf['Seccion_ID'].str.replace('SECTION ', '', regex=False)\n",
    "    df_pdf['Cap_Str'] = df_pdf['Capitulo_ID'].astype(str).str.zfill(2)\n",
    "    \n",
    "    mapa_cap_sec = pd.Series(df_pdf.Seccion_Romana.values, index=df_pdf.Cap_Str).to_dict()\n",
    "    df_secs = df_pdf[['Seccion_Romana', 'Seccion_Nombre']].drop_duplicates()\n",
    "    sec_names = pd.Series(df_secs.Seccion_Nombre.values, index=df_secs.Seccion_Romana).to_dict()\n",
    "    cap_names = pd.Series(df_pdf.Capitulo_Nombre.values, index=df_pdf.Cap_Str).to_dict()\n",
    "    \n",
    "    return sec_names, cap_names, mapa_cap_sec\n",
    "\n",
    "\n",
    "def procesar_csv_hts(csv_path):\n",
    "    print(\">> Procesando CSV Raw...\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        sys.exit(\"ERROR: No existe el CSV.\")\n",
    "\n",
    "    df = pd.read_csv(csv_path, dtype=str).iloc[:, :-3]\n",
    "    df = df.rename(columns={'HTS Number': 'HTS_Number', 'Indent': 'Indent', 'Description': 'Description',\n",
    "                            'Unit of Quantity': 'Unit', 'General Rate of Duty': 'General_Rate', \n",
    "                            'Special Rate of Duty': 'Special_Rate'})\n",
    "    \n",
    "    df = df.fillna('')\n",
    "    df['Indent'] = pd.to_numeric(df['Indent'], errors='coerce').fillna(0).astype(int)\n",
    "    df['HTS_Number'] = df['HTS_Number'].str.replace('.', '', regex=False).str.strip()\n",
    "\n",
    "    # Cortar en Capítulo 99\n",
    "    indices_99 = df.index[df['HTS_Number'].str.startswith('99', na=False)].tolist()\n",
    "    if indices_99: df = df.iloc[:indices_99[0]]\n",
    "\n",
    "    # Lógica de Indentación (Padre hereda a Hijo)\n",
    "    codes, indents = df['HTS_Number'].tolist(), df['Indent'].tolist()\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        if indents[i] == 1:\n",
    "            curr = str(codes[i]) if pd.notna(codes[i]) else \"\"\n",
    "            if (curr == \"\" or curr.lower() == 'nan') and i + 1 < n:\n",
    "                if indents[i+1] == 2 and str(codes[i+1]):\n",
    "                    codes[i] = str(codes[i+1])[:5]\n",
    "                    curr = codes[i]\n",
    "            if len(curr) > 5: codes[i] = curr[:5]\n",
    "        elif indents[i] == 0:\n",
    "            if len(str(codes[i])) > 4: codes[i] = str(codes[i])[:4]\n",
    "    \n",
    "    df['HTS_Number'] = codes\n",
    "    return df\n",
    "\n",
    "\n",
    "def generar_paneles(df_raw, sec_names, cap_names, mapa_cap_sec):\n",
    "    print(\">> Generando paneles analíticos...\")\n",
    "    \n",
    "    # Diccionarios internos (partidas, subpartidas)\n",
    "    d_part, d_des, d_sub = {}, {}, {}\n",
    "    for row in df_raw.itertuples():\n",
    "        c, d = str(row.HTS_Number), str(row.Description)\n",
    "        if not c: continue\n",
    "        if len(c) == 4: d_part[c] = d\n",
    "        elif len(c) == 5: d_des[c] = d\n",
    "        elif len(c) == 6: d_sub[c] = d\n",
    "\n",
    "    # Filtrar solo registros con Tasa General\n",
    "    df_fin = df_raw[df_raw['General_Rate'].notna() & (df_raw['General_Rate'] != '')].copy()\n",
    "    df_fin['HTS_Number'] = df_fin['HTS_Number'].apply(lambda x: str(x)[:8].ljust(8, '0'))\n",
    "    \n",
    "    p = df_fin.copy()\n",
    "    p['Codigo_Limpio'] = p['HTS_Number']\n",
    "    p['Codigo_Formato'] = p['Codigo_Limpio'].apply(_restaurar_puntos)\n",
    "    \n",
    "    # Mapeos Maestros\n",
    "    p['Capitulo_Num'] = p['Codigo_Limpio'].str[:2]\n",
    "    p['Seccion_Romana'] = p['Capitulo_Num'].apply(lambda x: mapa_cap_sec.get(x, \"ND\"))\n",
    "    p['Seccion_Nombre'] = p['Seccion_Romana'].map(sec_names)\n",
    "    p['Capitulo_Nom'] = p['Capitulo_Num'].map(cap_names)\n",
    "    \n",
    "    # Mapeos Internos\n",
    "    p['Partida_Nom'] = p['Codigo_Limpio'].str[:4].map(d_part)\n",
    "    p['Desdoblamiento_Nom'] = p['Codigo_Limpio'].str[:5].map(d_des)\n",
    "    p['Subpartida_Nom'] = p['Codigo_Limpio'].str[:6].map(d_sub)\n",
    "    \n",
    "    # Desglose Numérico\n",
    "    p['Partida_Num'] = p['Codigo_Limpio'].str[2:4]\n",
    "    p['Desdoblamiento_Num'] = p['Codigo_Limpio'].str[4:5]\n",
    "    p['Subpartida_Num'] = p['Codigo_Limpio'].str[5:6]\n",
    "    p['Fraccion_Num'] = p['Codigo_Limpio'].str[6:8]\n",
    "    \n",
    "    # Normalización de Texto (Capitalize + Puntos)\n",
    "    cols_txt = ['Seccion_Nombre', 'Capitulo_Nom', 'Partida_Nom', 'Desdoblamiento_Nom', 'Subpartida_Nom', 'Description']\n",
    "    for col in cols_txt:\n",
    "        p[col] = p[col].apply(_normalizar_texto)\n",
    "        \n",
    "    # Concatenación\n",
    "    p['Concatenated_Desc'] = p.apply(_concatenar_descripciones, axis=1)\n",
    "    \n",
    "    return p, df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_fase1_hts",
   "metadata": {},
   "source": [
    "### Fase 1: Extracción Estructural (PDF)\n",
    "En esta fase se invoca al motor de extracción para leer el PDF `Table of Contents`. El resultado es procesado por la función constructora para generar los diccionarios maestros que relacionan cada capítulo con su Sección y su descripción oficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "code_fase1_hts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [CRÍTICO] Leyendo estructura desde PDF...\n",
      ">> Construyendo diccionarios maestros...\n",
      "   Referencias cargadas: 99 capítulos detectados.\n"
     ]
    }
   ],
   "source": [
    "df_estructura = __procesar_pdf_estructura__(PATH_HTS_PDF)\n",
    "SEC_NAMES, CAP_NAMES, MAPA_CAP_SEC = construir_diccionarios_maestros(df_estructura)\n",
    "\n",
    "print(f\"   Referencias cargadas: {len(CAP_NAMES)} capítulos detectados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_fase2_hts",
   "metadata": {},
   "source": [
    "### Fase 2: Procesamiento de Datos (CSV)\n",
    "Se carga el archivo `htsdata.csv`. Se eliminan columnas innecesarias, se cortan los datos administrativos (Capítulo 99) y se ejecuta la lógica de indentación para asegurar que todos los registros tengan códigos válidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "code_fase2_hts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Procesando CSV Raw...\n",
      ">> Proceso finalizado.\n"
     ]
    }
   ],
   "source": [
    "DF_HTS_CLEAN = procesar_csv_hts(PATH_HTS_CSV)\n",
    "print(\">> Proceso finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_fase3_hts",
   "metadata": {},
   "source": [
    "### Fase 3: Construcción de Paneles\n",
    "Se cruza la información del CSV procesado con los diccionarios maestros extraídos del PDF. Se generan los diferentes DataFrames (Numérico, Textual, Extendido, Concatenado) aplicando las reglas de normalización de texto y concatenación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "code_fase3_hts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generando paneles analíticos...\n",
      ">> Paneles Generados.\n"
     ]
    }
   ],
   "source": [
    "PANEL_DF, DF_ORIGINAL = generar_paneles(DF_HTS_CLEAN, SEC_NAMES, CAP_NAMES, MAPA_CAP_SEC)\n",
    "\n",
    "# 1. Original Cleaned (Con puntos y Special Rate)\n",
    "df_orig = DF_ORIGINAL.copy()\n",
    "df_orig['HTS_Number'] = df_orig['HTS_Number'].apply(_restaurar_puntos)\n",
    "\n",
    "# 2. Numérico (Sin Special Rate)\n",
    "cols_num = ['Codigo_Formato', 'Seccion_Romana', 'Capitulo_Num', 'Partida_Num', 'Desdoblamiento_Num', 'Subpartida_Num', 'Fraccion_Num', 'Unit', 'General_Rate']\n",
    "df_num = PANEL_DF[cols_num].fillna('').copy()\n",
    "df_num.columns = ['Code', 'Section', 'Chapter', 'Heading', 'Breakdown', 'Subheading', 'Item', 'Unit', 'General Duty']\n",
    "\n",
    "# 3. Textual (Sin Special Rate)\n",
    "cols_txt = ['Codigo_Formato', 'Seccion_Nombre', 'Capitulo_Nom', 'Partida_Nom', 'Desdoblamiento_Nom', 'Subpartida_Nom', 'Description', 'Unit', 'General_Rate']\n",
    "df_txt = PANEL_DF[cols_txt].fillna('').copy()\n",
    "df_txt.columns = ['Code', 'Section Name', 'Chapter Name', 'Heading Name', 'Breakdown Name', 'Subheading Name', 'Description', 'Unit', 'General Duty']\n",
    "\n",
    "# 4. Extendido (Sin Special Rate)\n",
    "cols_ext = ['Codigo_Formato', 'Seccion_Romana', 'Seccion_Nombre', 'Capitulo_Num', 'Capitulo_Nom', 'Partida_Num', 'Partida_Nom', 'Desdoblamiento_Num', 'Desdoblamiento_Nom', 'Subpartida_Num', 'Subpartida_Nom', 'Fraccion_Num', 'Description', 'Unit', 'General_Rate']\n",
    "df_ext = PANEL_DF[cols_ext].fillna('').copy()\n",
    "df_ext.columns = ['Code', 'Section', 'Section Name', 'Chapter', 'Chapter Name', 'Heading', 'Heading Name', 'Breakdown', 'Breakdown Name', 'Subheading', 'Subheading Name', 'Item', 'Description', 'Unit', 'General Duty']\n",
    "\n",
    "# 5. Concatenado\n",
    "df_concat = PANEL_DF[['Codigo_Formato', 'Concatenated_Desc', 'Unit', 'General_Rate']].fillna('').copy()\n",
    "df_concat.columns = ['Code', 'Concatenated Description', 'Unit', 'General Duty']\n",
    "\n",
    "print(\">> Paneles Generados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_fase4_hts",
   "metadata": {},
   "source": [
    "### Fase 4: Exportación Final\n",
    "Se utiliza el motor `__escribir_hoja__` para generar el archivo Excel final, asegurando que las columnas de descripción y tasas especiales tengan el formato `Wrap Text` para una correcta visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "code_fase4_hts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando Excel: ../data/intermediate/HTS_Limpia.xlsx...\n",
      "¡ÉXITO! Archivo generado correctamente con estructura automatizada.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generando Excel: {PATH_SALIDA}...\")\n",
    "\n",
    "try:\n",
    "    writer = pd.ExcelWriter(PATH_SALIDA, engine='xlsxwriter')\n",
    "    workbook = writer.book\n",
    "\n",
    "    # Escritura de hojas\n",
    "    __escribir_hoja__(df_orig, \"Original Cleaned\", writer, workbook)\n",
    "    __escribir_hoja__(df_num, \"Numeric Panel\", writer, workbook)\n",
    "    __escribir_hoja__(df_txt, \"Textual Panel\", writer, workbook)\n",
    "    __escribir_hoja__(df_ext, \"Extended Panel\", writer, workbook)\n",
    "    __escribir_hoja__(df_concat, \"Concatenado\", writer, workbook)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"¡ÉXITO! Archivo generado correctamente con estructura automatizada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR CRÍTICO EN EXPORTACIÓN: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
